# LLMCoevaluator

## Introduction

This repository accompanies the article **“Coevaluando con LLM: generando una retroalimentación formativa a tiempo”** published to the *XXXI Jornadas sobre la Enseñanza Universitaria de la Informática (JENUI 2025)*. It proposes a methodology for co-evaluation between instructors and artificial intelligence in programming courses.

Formative feedback is a key pedagogical tool to foster reflection and meaningful learning. However, providing detailed and timely feedback in large classes is a significant burden for instructors.

To address this, we propose a solution based on **Large Language Models (LLMs)** acting as preliminary automatic evaluators. These models generate an initial assessment of the student's code, which is then reviewed and adjusted by the instructor. This reduces the time required to provide useful feedback without compromising quality or reliability. Note that fully automated grading is considered high-risk (EU AI Act 2024/1689), but this risk is mitigated when the process is supervised and modified by a human instructor.

This repository contains the code needed to perform this two-phase co-evaluation:

1. Automatic feedback generation using LLMs.
2. Instructor review and adjustment for final delivery in Moodle.

<p align="center">
    <img src="bigpicture.png" alt="System overview" width="20%">
</p>

<p align="center">
    <em>Figure 1: Proposed system for LLM-assisted co-evaluation.</em>
</p>

---

# Step 1: Installation and Setup

A `requirements.txt` file is included to simplify dependency installation in a new Python environment. It is recommended to create the environment in the root folder of the project.

## Create a Python Environment

To isolate dependencies and simplify installation, we recommend creating a Python virtual environment. Here are the steps for major operating systems:

### Windows

```sh
python -m venv venv
venv\Scripts\activate
pip install -r requirements.txt
```

### macOS / Linux

```sh
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```

Once the environment is activated, all dependencies will be installed locally and will not affect other projects.

You must also fill in your API keys in the `.env` file. This file is prepared to include access keys for OpenAI and Anthropic models. The system expects this file in the project root.

---

# Step 2: Automatic Evaluation with LLM

The project is organized to make it easy to extend with new LLM APIs and new types of assessments. Below is an overview of the folder structure and instructions for adding more APIs and assessments.

## Folder Structure

```
LLMCoevaluator/
├── EvaluateStudents.py
├── Assesments/
│   ├── BaseAssesment.py
│   ├── PolyC.py
│   ├── SplayTreeJava.py
│   └── ...
├── LLM/
│   ├── llm_base.py
│   └── API/
│       ├── anthropic.py
│       ├── openai.py
│       └── ...
```

- **EvaluateStudents.py**: Main script to run the evaluation process. This process expects a folder structure for student submissions. For example, you might have a directory such as `assesmentExampleC/`, which contains one subfolder per student. Each subfolder must be named exactly as the student appears in Moodle (this matches the structure when downloading submissions directly from Moodle). Inside each student's folder, the evaluation results generated by the LLM will be saved as `Result.html` files.
- **Assesments/**: Contains assessment logic for different exercises or languages.
  - `BaseAssesment.py`: Abstract base class for assessments.
  - Other files (e.g., `PolyC.py`, `SplayTreeJava.py`): Specific assessment implementations.
- **LLM/**: Logic for interacting with Large Language Models (LLMs).
  - `llm_base.py`: Base abstract class for LLM APIs. 
  - **API/**: Wrappers for different LLM providers:
    - `openai.py`: Wrapper for the OpenAI API. It allows you to select among the different available models (e.g., o3-mini, gpt-4o-mini, etc.) according to your evaluation needs. Please consult the [official OpenAI pricing page](https://platform.openai.com/docs/pricing) to check the current cost of API calls, as prices vary depending on the model and usage.
    `anthropic.py`: Wrapper for the Anthropic API. It allows you to select among the available models (e.g., claude-3-5-haiku-20241022, etc.) according to your evaluation needs. Please consult the [official Anthropic pricing page](https://www.anthropic.com/pricing#api) to check the current cost of API calls, as prices vary depending on the model and usage.

## How to Add More LLM APIs

1. **Create a new file in `LLM/API/`**  
   For example, to add a new provider, create `LLM/API/myllm.py`.

2. **Implement a class that inherits from `llm_base.py`**  
   Use the structure in `openai.py` or `anthropic.py` as a reference.

3. **Register or import your new API in the main evaluation logic**  
   Update de file `EvaluateStudents.py` where LLMs are selected to include your new provider.

## How to Add More Assessments

1. **Create a new file in `Assesments/`**  
   For example, `MyNewAssessment.py`.

2. **Implement a class that inherits from `BaseAssesment`**  
   See `PolyC.py` or `SplayTreeJava.py` for examples.

3. **Update the main script to use your new assessment**  
   Import and instantiate your new assessment class as needed.

**Summary:**  
- Add new LLM APIs in `LLM/API/` by subclassing `llm_base.py`.
- Add new assessments in `Assesments/` by subclassing `BaseAssesment.py`.

This modular structure makes it easy to extend the system for new providers and exercises.


---

## Co-evaluation

The `co_evaluator/` directory contains the code for the second phase: reviewing and adjusting the feedback generated by the LLM. The goal is to help instructors edit, validate, and personalize comments before uploading them to Moodle.

Before running the application, configure the parameters in `app.py` to match your data. Specifically, check and adjust the following variables at the top of the file:

```python
GRADES_FILE = "../java/submissions/grades.csv"  # Path to the CSV file exported from Moodle
CODE_EXTENSION = ".java"                        # Extension of code files to evaluate (e.g., ".java" or ".c")
BASE_FOLDER = "../java/submissions/"            # Folder containing one directory per student with their code files
```

Ensure the paths and extensions match your file organization and programming language.

### Running the Application

To start the co-evaluation app, run `app.py` in the `co_evaluator` folder:

```sh
python co_evaluator/app.py
```

The co-evaluation app is built with **Flask**, a lightweight Python web framework. Flask enables easy creation of interactive web interfaces for reviewing and adjusting feedback. Thanks to Flask, the tool provides a browser-accessible interface where instructors can manage LLM-generated comments before exporting them to Moodle.

By default, the app runs at [http://127.0.0.1:5000](http://127.0.0.1:5000). Open this URL in your browser after starting the server.

### How does it work?

1. **Load automatic results:** The co-evaluator imports the feedback files generated by the LLM for each student. It requires the CSV file with the grades exported from Moodle.
2. **Review interface:** Provides an interface (notebook or interactive script) where the instructor can review, modify, and approve the proposed comments.
3. **Export for Moodle:** As reviews and grade updates are made, the `grades.csv` file is updated. Once finished, this file can be imported back into Moodle to update student grades and comments.

---

# License and Contact

This work was developed at the University of Málaga by:

- Joaquín Ballesteros ([jballesteros@uma.es](mailto:jballesteros@uma.es))  [ITIS Software — CAOSD Group](https://itis.uma.es).

In collaboration with: 
- Pablo Franco ([pablo.franco@uma.es](mailto:pablo.franco@uma.es)) [Research Methods and Educational Diagnosis](https://www.uma.es/departamento-de-teoria-e-historia-de-la-educacion/)
- Lidia Fuentes ([lfuentes@uma.es](mailto:lfuentes@uma.es)) [ITIS Software — CAOSD Group](https://itis.uma.es).

If you use this code or methodology in your research or teaching, please cite the associated article.

---
